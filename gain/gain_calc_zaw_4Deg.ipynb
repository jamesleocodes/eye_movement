{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/two_degHC.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hj/4_1_bgn11_dgm99bq0lnjxl80000gn/T/ipykernel_3626/2586267868.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# data = pd.read_csv(data_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#import CSV data for stimulus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ds/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ds/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ds/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ds/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ds/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ds/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/two_degHC.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Tuesday 1 May 2023\n",
    "Author: ZAW\n",
    "\"\"\"\n",
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "### Load the subject and stiumuls repect to their speed\n",
    "path = os.getcwd()\n",
    "dirname = os.path.dirname(path)\n",
    "# for HC\n",
    "data_file = 'data/four_degHC.csv'\n",
    "# # for PD\n",
    "#data_file = 'data/PD/one_deg.csv'\n",
    "\n",
    "data_path = os.path.join(dirname,data_file)\n",
    "# data = pd.read_csv(data_path)\n",
    "data= pd.read_csv(data_path)\n",
    "\n",
    "#import CSV data for stimulus\n",
    "path_sti = os.getcwd()\n",
    "dirname_sti = os.path.dirname(path_sti)\n",
    "data_file_sti = 'data/4_degSti.xlsx'\n",
    "data_path_sti = os.path.join(dirname_sti,data_file_sti)\n",
    "rawdata = pd.read_excel(data_path_sti)\n",
    "data_st = np.array(rawdata[1:],dtype=np.float)\n",
    "time = data_st[:,1]\n",
    "position = data_st[:,0]\n",
    "\n",
    "FirstTrialHC = []\n",
    "\n",
    "# List of subject codes (e.g. PD001, PD002, etc.)\n",
    "subject_codes = list(set([col[3:] for col in data.columns if col.startswith('x1_')]))\n",
    "\n",
    "for subject_code in subject_codes:\n",
    "    ### Load the subject and stiumuls repect to their speed\n",
    "    path = os.getcwd()\n",
    "    dirname = os.path.dirname(path)\n",
    "    # for HC\n",
    "    data_file = 'data/four_degHC.csv'\n",
    "    # # for PD\n",
    "    #data_file = 'data/PD/one_deg.csv'\n",
    "\n",
    "    data_path = os.path.join(dirname,data_file)\n",
    "    # data = pd.read_csv(data_path)\n",
    "    data= pd.read_csv(data_path)\n",
    "    print('File '+ str(subject_code)+ ' is started.') \n",
    "    x_column = f'x1_{subject_code}'\n",
    "    y_column = f'y1_{subject_code}'\n",
    "\n",
    "    # Check if both columns exist in the DataFrame before accessing them\n",
    "    if x_column in data.columns and y_column in data.columns:\n",
    "        data_hc = data[[x_column, y_column]]\n",
    "    else:\n",
    "        print(f\"Columns for subject code {subject_code} not found. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    y_data = data_hc[x_column]\n",
    "    x_data = data_hc[y_column]\n",
    "\n",
    "    # data_hc = data[['x1_PD001','y1_PD001']] # change the ID each subject code(e.g PD001 or PD003)\n",
    "    # y_data = data_hc[data_hc.columns[0]]\n",
    "    # x_data = data_hc[data_hc.columns[1]]\n",
    "\n",
    "\n",
    "    # Trigonometric Functions\n",
    "    # Trigonometric functions\n",
    "\n",
    "    # Define the function\n",
    "    def func(x, a, b, c):  #Position as a function of time.\n",
    "        return a*(2/np.pi)*np.arcsin(np.sin(np.pi*(b*x+c)))\n",
    "\n",
    "    #initial guesses( This has to change depend on stimulus speed)\n",
    "    #for 1 degree per second: [10, 0.05, 0]\n",
    "    #for 2 degrees per second: [10, 0.1, 0]\n",
    "    #for 4 degrees per second: [10, 0.2, 0]\n",
    "    #for 6 degrees per second: [10, 0.3, 0]\n",
    "    #for 8 degrees per second: [10, 0.4, 0]\n",
    "    InitialGuess = [10, 0.2, 0] # for one degree\n",
    "\n",
    "    # Perform curve fitting\n",
    "    popt, pcov = curve_fit(func,position,time, p0=InitialGuess)\n",
    "\n",
    "    # Extract the optimal values of a, b, and c\n",
    "    a, b, c = popt\n",
    "    # print(\"a =\", a)\n",
    "    # print(\"b =\", b)\n",
    "    # print(\"c =\", c)\n",
    "\n",
    "    # Fit the curve\n",
    "    fit_time = func(y_data,a,b,c)\n",
    "    # plt.plot(y_data,fit_time)\n",
    "    # plt.show()\n",
    "\n",
    "    # Find the residual\n",
    "    # Different(aka residual)\n",
    "    diff = x_data - fit_time\n",
    "\n",
    "    dt_array = np.array(diff)\n",
    "    dt_array = pd.DataFrame(dt_array,columns=['diff'])\n",
    "    window_size = 2\n",
    "    dt_array['Moving_Average'] = dt_array['diff'].rolling(window=window_size).mean()\n",
    "    #plt.plot(y_data,dt_array['Moving_Average'])\n",
    "    # create dataframe\n",
    "    data = {'Time':y_data,'POS':dt_array['Moving_Average']}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # The whole dataset\n",
    "    # Plot specific range\n",
    "    x_start = 10.01\n",
    "    x_end = 90.00\n",
    "\n",
    "    # Filter the data points within the disired range using boolean indexing\n",
    "    mask = (df['Time'] >= x_start) & (df['Time'] <= x_end)\n",
    "    x_data_range = df.loc[mask]\n",
    "\n",
    "    data = {'Time':x_data_range['Time'],'POS':x_data_range['POS']}\n",
    "    df_test = pd.DataFrame(data)\n",
    "\n",
    "    # Determine the integration window\n",
    "    def check_sign(x):\n",
    "        if x > 0:\n",
    "            return \"Positive\"\n",
    "        else:\n",
    "            return \"Negative\"\n",
    "    df_test['Sign'] = df_test['POS'].apply(check_sign)\n",
    "\n",
    "    # Condition window width \n",
    "    def calculate_start_end(row):\n",
    "        global last_positive, last_negative\n",
    "        if row['Sign'] == 'Positive':\n",
    "            if row.name == 0 or df_test.loc[row.name - 1, 'Sign'] != 'Positive':\n",
    "                last_positive = row['Time']\n",
    "                return last_positive, '', '', ''\n",
    "            elif row.name < len(df_test) - 1 and df_test.loc[row.name + 1, 'Sign'] != 'Positive':\n",
    "                pos_end = row['Time']\n",
    "                last_positive = ''\n",
    "                return '', pos_end, '', ''\n",
    "            else:\n",
    "                return '', '', '', ''\n",
    "        elif row['Sign'] == 'Negative':\n",
    "            if row.name == 0 or df_test.loc[row.name - 1, 'Sign'] != 'Negative':\n",
    "                last_negative = row['Time']\n",
    "                return '', '', last_negative, ''\n",
    "            elif row.name < len(df_test) - 1 and df_test.loc[row.name + 1, 'Sign'] != 'Negative':\n",
    "                last_negative = row['Time']\n",
    "                return '', '', '', last_negative\n",
    "            else:\n",
    "                return '', '', '', ''\n",
    "        else:\n",
    "            return '', '', '', ''\n",
    "\n",
    "    # Initialize the last positive and negative values to empty strings \n",
    "    last_positive = ''\n",
    "    last_negative = ''\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    # Apply the custom function to create new columns\n",
    "    df_test['PosTim_Start'], df_test['PosTim_End'], df_test['NegTim_Start'], df_test['NegTim_End'] = zip(*df_test.apply(calculate_start_end, axis=1))\n",
    "\n",
    "    # Fill the empty cells with an empty string\n",
    "    df_test['PosTim_Start'] = df_test['PosTim_Start'].fillna('') \n",
    "    df_test['PosTim_End'] = df_test['PosTim_End'].fillna('')\n",
    "    df_test['NegTim_Start'] = df_test['NegTim_Start'].fillna('')\n",
    "    df_test['NegTim_End'] = df_test['NegTim_End'].fillna('')\n",
    "\n",
    "    # Print the resulting dataframe \n",
    "    #print(df_test)\n",
    "\n",
    "\n",
    "    # Positive time start trimming\n",
    "    # Select the non-empty values in the 'Pos_Start' column\n",
    "    pos_start_values = df_test.loc[df_test['PosTim_Start'] != '', 'PosTim_Start'].values\n",
    "\n",
    "    # Round the values in the 'Pos_Start' column to two decimal places\n",
    "    # rounded_pos_start_values = []\n",
    "    # for value in pos_start_values:\n",
    "    #     rounded_pos_start_values.append(round(value - 0.003, 2))\n",
    "\n",
    "    # Convert the list of rounded values back to a NumPy array\n",
    "    postim_start_values = np.array(pos_start_values)\n",
    "\n",
    "    # Print the non-empty values\n",
    "    #print(postim_start_values)\n",
    "\n",
    "\n",
    "    # Positive time end triming\n",
    "    # Select the non-empty values in the 'Pos_End' column\n",
    "    pos_end_values = df_test.loc[df_test['PosTim_End'] != '', 'PosTim_End'].values\n",
    "\n",
    "    # Round the values in the 'Pos_End' column to two decimal places\n",
    "    # rounded_pos_end_values = []\n",
    "    # for value in pos_end_values:\n",
    "    #     rounded_pos_end_values.append(round(value - 0.02, 2))\n",
    "\n",
    "    # Convert the list of rounded values back to a NumPy array\n",
    "    postim_end_values = np.array(pos_end_values)\n",
    "\n",
    "    # Print the non-empty values\n",
    "    #print(postim_end_values)\n",
    "\n",
    "\n",
    "    # Reload the raw data for mapping\n",
    "    data = data_hc.rename(columns={data_hc.columns.values[0]:\"Position\",\n",
    "                                data_hc.columns.values[1]:'Time'})\n",
    "\n",
    "\n",
    "    # Map the start positive time to POS in raw data\n",
    "    # Create a dictionary from the mapping list\n",
    "    mapping_list = postim_start_values\n",
    "\n",
    "    # Map the values to the 'POS' column in raw\n",
    "    mapped_posSt = data.loc[data['Position'].isin(mapping_list),'Time']\n",
    "\n",
    "\n",
    "    # Map the end positive time to POS in raw data\n",
    "    # Create a dictionary from the mapping list\n",
    "    mapping_list = postim_end_values\n",
    "\n",
    "    # Map the values to the 'POS' column in raw\n",
    "    mapped_posEnd = data.loc[data['Position'].isin(mapping_list),'Time']\n",
    "\n",
    "\n",
    "    # Velocity param positive dataframe\n",
    "    # # column miss match\n",
    "    # Check lengths and truncate longer column \n",
    "    if len(postim_start_values) > len(mapped_posSt): \n",
    "        postim_start_values = postim_start_values[:len(mapped_posSt)] \n",
    "    elif len(mapped_posSt) > len(postim_start_values):\n",
    "        mapped_posSt = mapped_posSt[:len(postim_start_values)]      \n",
    "\n",
    "    # Now columns have equal length \n",
    "    data_param = {'positive_x1':postim_start_values,'positive_y1':mapped_posSt}             \n",
    "    param_vel_start = pd.DataFrame(data_param)\n",
    "\n",
    "    data_param= {'positive_x2':postim_end_values,'positive_y2':mapped_posEnd}\n",
    "    param_vel_end = pd.DataFrame(data_param)\n",
    "\n",
    "    # Reset the indices of the DataFrames\n",
    "    param_vel_start = param_vel_start.reset_index(drop=True)\n",
    "    param_vel_end = param_vel_end.reset_index(drop=True)\n",
    "\n",
    "    # Concatenate the DataFrames horizontally\n",
    "    param_vel_positive_final = pd.concat([param_vel_start,param_vel_end],axis=1)\n",
    "    #param_vel_positive_final['Del_X'] = param_vel_positive_final['positive_x2'] - param_vel_positive_final['positive_x1']\n",
    "\n",
    "    param_vel_positive_final['Del_X'] = (param_vel_positive_final['positive_x2'] - \n",
    "                                        param_vel_positive_final['positive_x1']).where(~param_vel_positive_final['positive_x2'].isnull(), np.nan)\n",
    "\n",
    "\n",
    "\n",
    "    # # Remove short duration SWJ from 50 ms to 400 ms\n",
    "    param_vel_positive_final = param_vel_positive_final[(param_vel_positive_final['Del_X'] > 0.4) & \n",
    "                                                        (param_vel_positive_final['Del_X'] < 2.00)]\n",
    "\n",
    "\n",
    "    # iterate over the rows of the dataframe\n",
    "    prev_positive_x2 = None\n",
    "    for i, row in param_vel_positive_final.iterrows():\n",
    "        # check if this is not the first row\n",
    "        if prev_positive_x2 is not None:\n",
    "            # check if positive_x1 is lower than the previous positive_x2\n",
    "            if row['positive_x1'] < prev_positive_x2:\n",
    "                # remove this row from the dataframe\n",
    "                param_vel_positive_final = param_vel_positive_final.drop(i)\n",
    "            else:\n",
    "                # update prev_positive_x2 if positive_x1 is greater than or equal to prev_positive_x2\n",
    "                prev_positive_x2 = row['positive_x2']\n",
    "        else:\n",
    "            # initialize prev_positive_x2 with the first row value\n",
    "            prev_positive_x2 = row['positive_x2']\n",
    "\n",
    "\n",
    "    # Velocity for positive pieak calculation\n",
    "    # Calculate the difference between y2 and y1 divided by the difference between x2 and x1\n",
    "    param_vel_positive_final['slope'] = (param_vel_positive_final['positive_y2'] - param_vel_positive_final['positive_y1']) / (param_vel_positive_final['positive_x2'] - param_vel_positive_final['positive_x1'])\n",
    "\n",
    "    # abs\n",
    "    param_vel_positive_final['slope'] = abs(param_vel_positive_final['slope'])\n",
    "\n",
    "    # Reindexing\n",
    "    param_vel_positive_final = param_vel_positive_final.reset_index(drop=True)\n",
    "\n",
    "    # Remove unwant values\n",
    "    #param_gain_positive_final.loc[param_gain_positive_final['slope'] > 1.09, 'slope'] = np.nan  \n",
    "\n",
    "    # Calculate the average slope\n",
    "    average_positive_slope = abs(param_vel_positive_final['slope'].mean())\n",
    "\n",
    "    # Print the average slope\n",
    "    #print(average_positive_slope)\n",
    "\n",
    "\n",
    "    # Negative time start trimming\n",
    "    # Select the non-empty values in the 'Pos_Start' column\n",
    "    neg_start_values = df_test.loc[df_test['NegTim_Start'] != '', 'NegTim_Start'].values\n",
    "\n",
    "    # Round the values in the 'Pos_Start' column to two decimal places\n",
    "    rounded_neg_start_values = []\n",
    "    for value in neg_start_values:\n",
    "        rounded_neg_start_values.append(round(value - 0.003, 2))\n",
    "\n",
    "    # Convert the list of rounded values back to a NumPy array\n",
    "    negtim_start_values = np.array(rounded_neg_start_values)\n",
    "\n",
    "    # Print the non-empty values\n",
    "    #print(negtim_start_values)\n",
    "\n",
    "\n",
    "    # Negative Time End Triming\n",
    "    # Select the non-empty values in the 'Pos_Start' column\n",
    "    neg_end_values = df_test.loc[df_test['NegTim_End'] != '', 'NegTim_End'].values\n",
    "\n",
    "    # Round the values in the 'Pos_Start' column to two decimal places\n",
    "    # rounded_neg_end_values = []\n",
    "    # for value in neg_end_values:\n",
    "    #     rounded_neg_end_values.append(round(value - 0.02, 2))\n",
    "\n",
    "    # Convert the list of rounded values back to a NumPy array\n",
    "    negtim_end_values = np.array(neg_end_values)\n",
    "\n",
    "\n",
    "    # Map the start negative time to POS in raw data\n",
    "    # Create a dictionary from the mapping list\n",
    "    mapping_list = negtim_start_values\n",
    "\n",
    "    # Map the values to the 'POS' column in raw\n",
    "    mapped_negSt = data.loc[data['Position'].isin(mapping_list),'Time']\n",
    "\n",
    "\n",
    "\n",
    "    # Ma the end of negative time to pos in raw data\n",
    "    # Create a dictionary from the mapping list\n",
    "    mapping_list = negtim_end_values\n",
    "\n",
    "    # Map the values to the 'POS' column in raw\n",
    "    mapped_negEnd = data.loc[data['Position'].isin(mapping_list),'Time']\n",
    "\n",
    "\n",
    "    # Velocity param negative datafrmae\n",
    "    # column miss match\n",
    "    # Check lengths and truncate longer column \n",
    "    if len(negtim_start_values) > len(mapped_negSt): \n",
    "        negtim_start_values = negtim_start_values[:len(mapped_negSt)] \n",
    "    elif len(mapped_negSt) > len(negtim_start_values):\n",
    "        mapped_negSt = mapped_negSt[:len(negtim_start_values)]  \n",
    "\n",
    "\n",
    "    data_param = {'negative_x1':negtim_start_values,'negative_y1':mapped_negSt}             \n",
    "    param_vel_start = pd.DataFrame(data_param)\n",
    "    data_param= {'negative_x2':negtim_end_values,'negative_y2':mapped_negEnd}\n",
    "    param_vel_end = pd.DataFrame(data_param)\n",
    "\n",
    "    # Reset the indices of the DataFrames\n",
    "    param_vel_start = param_vel_start.reset_index(drop=True)\n",
    "    param_vel_end = param_vel_end.reset_index(drop=True)\n",
    "\n",
    "    # Concatenate the DataFrames horizontally\n",
    "    param_vel_negative_final = pd.concat([param_vel_start,param_vel_end],axis=1)\n",
    "    param_vel_negative_final['Del_X'] = param_vel_negative_final['negative_x2'] - param_vel_negative_final['negative_x1']\n",
    "    #param_vel_negative_final = param_vel_negative_final.fillna(method='ffill')\n",
    "    #param_vel_negative_final\n",
    "\n",
    "    # Remove short duration\n",
    "    # param_vel_negative_final = param_vel_negative_final[(param_vel_negative_final['Del_X'] > 0.3) & \n",
    "    #                                                     (param_vel_negative_final['Del_X'] < 2.07)]\n",
    "    param_vel_negative_final = param_vel_negative_final[param_vel_negative_final['Del_X'] < 2.07]\n",
    "    #                                                     \n",
    "    param_vel_negative_final['Del_X'] = (param_vel_negative_final['negative_x2'] - \n",
    "                                        param_vel_negative_final['negative_x1']).where(~param_vel_negative_final['negative_x2'].isnull(), np.nan)\n",
    "\n",
    "\n",
    "\n",
    "    # # Remove short duration SWJ from 50 ms to 400 ms\n",
    "    param_vel_negative_final = param_vel_negative_final[(param_vel_negative_final['Del_X'] > 0.4) & \n",
    "                                                        (param_vel_negative_final['Del_X'] < 2.00)]\n",
    "\n",
    "\n",
    "    # iterate over the rows of the dataframe\n",
    "    prev_negative_x2 = None\n",
    "    for i, row in param_vel_negative_final.iterrows():\n",
    "        # check if this is not the first row\n",
    "        if prev_negative_x2 is not None:\n",
    "            # check if positive_x1 is lower than the previous positive_x2\n",
    "            if row['negative_x1'] < prev_negative_x2:\n",
    "                # remove this row from the dataframe\n",
    "                param_vel_negative_final = param_vel_negative_final.drop(i)\n",
    "            else:\n",
    "                # update prev_positive_x2 if positive_x1 is greater than or equal to prev_positive_x2\n",
    "                prev_megatove_x2 = row['negative_x2']\n",
    "        else:\n",
    "            # initialize prev_positive_x2 with the first row value\n",
    "            prev_negative_x2 = row['negative_x2']\n",
    "\n",
    "    # Velocity for Negative peak calculation\n",
    "    # Calculate the difference between y2 and y1 divided by the difference between x2 and x1\n",
    "    param_vel_negative_final['slope'] = (param_vel_negative_final['negative_y2']\n",
    "                                        - param_vel_negative_final['negative_y1']) / (param_vel_negative_final['negative_x2'] \n",
    "                                        - param_vel_negative_final['negative_x1'])\n",
    "\n",
    "    # abs\n",
    "    param_vel_negative_final['slope'] = abs(param_vel_negative_final['slope'])\n",
    "\n",
    "    param_vel_negative_final['slope'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Reindexing\n",
    "    param_vel_negative_final = param_vel_negative_final.reset_index(drop=True)\n",
    "\n",
    "    # Remove unwant values\n",
    "    #param_gain_negative_final.loc[(param_gain_negative_final['slope'] < 0.80) |  \n",
    "    #                              (param_gain_negative_final['slope'] > 1.09), \n",
    "    #                              'slope'] = np.nan\n",
    "    # Calculate the average slope\n",
    "    average_negative_slope = abs(param_vel_negative_final['slope'].mean())\n",
    "\n",
    "    # Print the average slope\n",
    "    #print(average_negative_slope)\n",
    "\n",
    "\n",
    "    # Total velocity for both positive and negative\n",
    "    Final_Vel = pd.concat([param_vel_positive_final[['Del_X','slope']],\n",
    "                        param_vel_negative_final[['Del_X','slope']]], axis=0, join='outer')\n",
    "\n",
    "    # Counter chck with duration and velocity\n",
    "    Final_Vel['Checked_velocity'] = Final_Vel['slope'] * Final_Vel['Del_X']\n",
    "    Vel = Final_Vel['Checked_velocity'].sum()/Final_Vel['Del_X'].sum()\n",
    "\n",
    "    # Calculate gain\n",
    "    speed = 4 # stimulus speed\n",
    "    total_gain = Vel/speed\n",
    "    #print(\"The gain for this subject is:\",total_gain)\n",
    "    result = {\"SubjectID\":subject_code ,\"gain\":total_gain}\n",
    "    FirstTrialHC.append(result)\n",
    "    print('File '+ str(subject_code)+ ' is done.') \n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "FirstTrialHC_df = pd.DataFrame(FirstTrialHC)\n",
    "FirstTrialHC_df .to_csv('FourFirstTrialHC_df .csv',index=False)\n",
    "#print(FirstTrialHC_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
